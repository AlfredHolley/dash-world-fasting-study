{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data have been downloaded here : <a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0209353\">Plos One - World largest Study</a><br>\n",
    "From data available only tables :\n",
    "<ul>\n",
    "    <li>S8 (baseline : metadata)</li>\n",
    "    <li>S10 (blood cells parameters)</li>\n",
    "    <li>S11 (blood parameters)</li>\n",
    "    <li>S12 (weight, blood pressure, well being, ketones)</li>\n",
    "    <li>S15 (lipid and glycemia parameters)</li>\n",
    "</ul>\n",
    "Were downloaded, a little pre-processing was done directly in Excel :\n",
    "<ul>\n",
    "    <li>keeping only the id and parameters, except for the metadata</li>\n",
    "    <li>removing header and footer lines which were notes, from each file</li>\n",
    "    <li>renaming the columns \"lenght of fast (days)\" by \"length_of_fast\"</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Load the data\n",
    "blood_cell           = pd.read_excel('blood_cell.xlsx')\n",
    "blood_param          = pd.read_excel('blood_param.xlsx')\n",
    "lipid_glucose        = pd.read_excel('lipid_glucose.xlsx')\n",
    "metadata             = pd.read_excel('metadata.xlsx')\n",
    "weight_sbp_wb_ketone = pd.read_excel('weight_sbp_wb_ketones.xlsx')\n",
    "\n",
    "# Merge the dataframes on 'id' column\n",
    "df_merge = (metadata\n",
    "            .merge(weight_sbp_wb_ketone, on='id')\n",
    "            .merge(lipid_glucose, on='id')\n",
    "            .merge(blood_cell, on='id')\n",
    "            .merge(blood_param, on='id'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.query(\"age.between(@age)\").index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([1,2,3,3]).intersection(set([3,4,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning : \n",
    "<ul>\n",
    "    <li>transform the dataframe to a long format adding a column 'timepoint'</li>\n",
    "    <li>cast columns to the right format replacing \"<\" by \"\" or \",\" by \".\"</li>\n",
    "    <li>replace strong outliers by missing values</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the extra spaces in the column names\n",
    "df_merge.columns = df_merge.columns.str.replace('  ', ' ')\n",
    "\n",
    "# Get the columns ending with \"pre\" or \"post\"\n",
    "columns_to_melt = df_merge.filter(regex='pre$|post$').columns\n",
    "# Get metadata columns\n",
    "id_vars_col = metadata.columns.tolist()\n",
    "\n",
    "# Melt the dataframe\n",
    "df_long = pd.melt(df_merge, \n",
    "                  id_vars=id_vars_col, \n",
    "                  value_vars=columns_to_melt, \n",
    "                  var_name='parameter')\n",
    "\n",
    "## Extract the timepoint based on the suffix of the parameter column\n",
    "df_long['timepoint'] = (df_long['parameter'].str\n",
    "                        .endswith('post').astype(int))\n",
    "\n",
    "## Extract the value name based on what comes before \"pre\" or \"post\"\n",
    "df_long['parameter'] = (df_long['parameter'].str\n",
    "                        .replace(r'\\s*(pre|post)$', '', regex=True))\n",
    "\n",
    "# Rename the value column in order to have like a categorical variable\n",
    "df_long.timepoint = (df_long[\"timepoint\"].astype(str)\n",
    "                     .replace({\"0\":\"pre\",\"1\":\"post\"}))\n",
    "\n",
    "# Replace the \"<\" by \"\" in the value column to cast the columns as numeric\n",
    "df_long.loc[:,\"value\"] = (df_long[\"value\"].astype(str)\n",
    "                          .replace({\"<\": \"\", \",\":\".\"}, regex=True))\n",
    "df_long[\"value\"] = pd.to_numeric(df_long[\"value\"], errors=\"coerce\")\n",
    "\n",
    "# replace strong outliers by np.nan, defined as values that are >= 5 std \n",
    "# from the mean of each group (parameter, timepoint)\n",
    "df_long['value'] = (df_long\n",
    "                    .groupby(['parameter', 'timepoint'])['value']\n",
    "                    .transform(\n",
    "    lambda x : x.mask(np.abs(((x - x.mean()) / x.std())).ge(5), np.nan)\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_parquet('merged_data_wide.parquet')\n",
    "df = df_raw.reset_index(drop = True).set_index(\"id\").copy()\n",
    "df0 =df.query(\"timepoint.eq(0)\").copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5, 6, 8, 20, 23, 28}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age = [18,50]\n",
    "index = set(df0[df0[\"age (years)\"].between(age[0], age[1])].index)\n",
    "a = {5,6,8,20,23,28}.intersection(index)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Addition of informations in order to prepare the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addition of a numeric column indicating exactly the timepoint.\n",
    "df_long[\"timeline\"] = np.where(df_long[\"timepoint\"] == \"pre\", 0, df_long[\"fasting duration (days)\"])\n",
    "\n",
    "df_long.sort_values([\"parameter\", \"id\", \"timeline\"], inplace=True)\n",
    "df_long[\"value_change\"] = (df_long\n",
    "                           .groupby([\"id\", \"parameter\"])[\"value\"]\n",
    "                           .transform(\"diff\"))\n",
    "# we need to have the change in all rows of ('id', 'parameter') groups for the following steps\n",
    "# using .bfill() or .ffill() introduce errors in the data, we prefer to use \"first\" in a transform function.\n",
    "df_long[\"value_change\"] = df_long.groupby([\"id\", \"parameter\"])[\"value_change\"].transform(\"first\")\n",
    "\n",
    "# it still some outliers for the change, after having a look on it, \n",
    "# we decided to remove the values that are >= 8 std from the mean of each group (parameter, timepoint)\n",
    "outlier_change_mask = (\n",
    "    df_long\n",
    "    .groupby(['parameter', 'timepoint'])['value_change']\n",
    "    .transform(lambda x: np.abs((x - x.mean()) / x.std()).ge(7))\n",
    "    )\n",
    "df_long.loc[outlier_change_mask, 'value_change'] = np.nan\n",
    "df_long.loc[outlier_change_mask, 'value'] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide = df_long.pivot_table(index=['id', 'timepoint', 'sex', 'fasting duration (days)', 'age (years)'], columns=\"parameter\", values=[\"value\", \"value_change\"], ).reset_index()\n",
    "df_wide.columns = [\" \".join(col) for col in df_wide.columns.values]\n",
    "df_wide.columns = [col[6:] if col.startswith(\"value \") else col for col in df_wide.columns.values]\n",
    "df_wide.columns = [col[13:] + \" change\" if col.startswith(\"value_change\") else col for col in df_wide.columns.values]\n",
    "df_wide.columns = [col.strip() for col in df_wide.columns.values]\n",
    "df_wide[\"timepoint\"] = df_wide[\"timepoint\"].replace({\"pre\":0, \"post\":1})\n",
    "\n",
    "# it will be usefill for having a jittered x axis on timepoint.\n",
    "df_wide['jittered_x'] = (\n",
    "    df_wide['timepoint']\n",
    "    .apply(lambda x : x + round(np.random.uniform(-0.1, 0.1),2))\n",
    ")\n",
    "df_wide.sort_values(by=['timepoint','id'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export the cleaned dataframe to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long.to_parquet('merged_data_long.parquet', index=False)\n",
    "df_wide.to_parquet('merged_data_wide.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### draw the plot of correlation exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = pd.read_excel(\"correlation_matrix.xlsx\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_param = correlation_matrix.columns.to_list()\n",
    "param = list_of_param[0]\n",
    "filtered_matrix = correlation_matrix.loc[:,param].reset_index().set_index(\"index\").fillna(0)\n",
    "list_of_param =  filtered_matrix.index.to_list()\n",
    "filtered_matrix.sort_values(by = param,inplace=True)\n",
    "# fig = go.Figure(data=go.Heatmap(\n",
    "#             z=filtered_matrix.values,\n",
    "#             y=filtered_matrix.index,\n",
    "#             x=filtered_matrix.columns,\n",
    "#             colorscale='RdBu',\n",
    "#             colorbar=dict(title='', orientation=\"v\", thickness=15,tickfont=dict(size=12)),\n",
    "#             zmin = -1, zmax = 1,\n",
    "#             hoverongaps=False,\n",
    "#             hoverinfo='none',  # Disable hovering\n",
    "#             text=[[name] for name in list(filtered_matrix.index)],\n",
    "#             texttemplate=\"%{text}\",\n",
    "#             textfont={\"size\":10},\n",
    "#         ))\n",
    "# fig.update_layout(\n",
    "#         margin=dict(l=20, r=20, t=50, b=0),\n",
    "#         yaxis=dict(title=\"\", showticklabels=False),\n",
    "#         xaxis=dict(title=\"\", showticklabels=False),\n",
    "#         width=300,\n",
    "#         height=1100,\n",
    "#         hovermode=\"x\",\n",
    "#     )\n",
    "# fig.update_xaxes(title=f\"Correlation : {param}\")\n",
    "# print(fig.layout.xaxis.hoverformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = pd.DataFrame({\"index\" :[\"ref1\", \"ref_neg1\"], \"value\": [1,-1]}).set_index(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = list(filtered_matrix.iloc[:,0].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = pd.read_excel(\"correlation_matrix.xlsx\", index_col=0)\n",
    "json_matrix = [correlation_matrix.to_dict()]\n",
    "filtered_data_dict = {key: round(value,2) for key, value in json_matrix[0].get(\"baseline of the parameter\").items() if not str(value) ==\"nan\"}\n",
    "sorted_data_dict = dict(sorted(filtered_data_dict.items(), key=lambda item: abs(item[1]), reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, -0.8991413404613746],\n",
       " [0, 1, -0.8515919761969845],\n",
       " [0, 2, -0.6558393097409405],\n",
       " [0, 3, -0.6525243285992586],\n",
       " [0, 4, -0.6498552500383081],\n",
       " [0, 5, -0.6213175249306229],\n",
       " [0, 6, -0.5952905061690204],\n",
       " [0, 7, -0.5733215536568387],\n",
       " [0, 8, -0.57083442689596],\n",
       " [0, 9, -0.5278778220222644],\n",
       " [0, 10, -0.5041531234010463],\n",
       " [0, 11, -0.4853838881793094],\n",
       " [0, 12, -0.4810645415272526],\n",
       " [0, 13, -0.4709228715750567],\n",
       " [0, 14, -0.4569667637006651],\n",
       " [0, 15, -0.4367197786930928],\n",
       " [0, 16, -0.4360541648249073],\n",
       " [0, 17, -0.4205220162706504],\n",
       " [0, 18, -0.4145689886180869],\n",
       " [0, 19, -0.4130390807728215],\n",
       " [0, 20, -0.4064701066488774],\n",
       " [0, 21, -0.4033050373125016],\n",
       " [0, 22, -0.3873264619774134],\n",
       " [0, 23, -0.364329816035065],\n",
       " [0, 24, -0.3295898762878696],\n",
       " [0, 25, -0.3256605992658447],\n",
       " [0, 26, -0.2985249529862553],\n",
       " [0, 27, -0.2840026766471113],\n",
       " [0, 28, -0.2614921946477595],\n",
       " [0, 29, -0.2576834764839997],\n",
       " [0, 30, -0.2527938448905704],\n",
       " [0, 31, -0.2494749645808349],\n",
       " [0, 32, -0.2353626854766831],\n",
       " [0, 33, -0.2204459802400731],\n",
       " [0, 34, -0.1714119396077234],\n",
       " [0, 35, -0.09676970423723355],\n",
       " [0, 36, -0.0508062985944454],\n",
       " [0, 37, -0.03479053445149029],\n",
       " [0, 38, -0.01643107533584176],\n",
       " [0, 39, 0.0],\n",
       " [0, 40, 0.0],\n",
       " [0, 41, 0.0],\n",
       " [0, 42, 0.05043655630872507],\n",
       " [0, 43, 0.2296334923455308]]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "param = 'baseline of the parameter'\n",
    "filtered_matrix = correlation_matrix.loc[:,param].reset_index().set_index(\"index\").fillna(0)\n",
    "list_of_param =  filtered_matrix.index.to_list()\n",
    "filtered_matrix.sort_values(by = param,inplace=True)\n",
    "\n",
    "hours = [param]\n",
    "days = list_of_param\n",
    "data = list(filtered_matrix.iloc[:,0].values)\n",
    "\n",
    "options = {\n",
    "    'tooltip': {\n",
    "        'position': 'top',\n",
    "    },\n",
    "    'grid': {\n",
    "        'height': '50%',\n",
    "        'top': '10%'\n",
    "    },\n",
    "    'xAxis': {\n",
    "        'type': 'category',\n",
    "        'data': [param],\n",
    "        'splitArea': {\n",
    "            'show': True\n",
    "        }\n",
    "    },\n",
    "    'yAxis': {\n",
    "        'show': False,\n",
    "    },\n",
    "    'visualMap': {\n",
    "        'min': 0,\n",
    "        'max': 10,\n",
    "        'calculable': True,\n",
    "        'orient': 'horizontal',\n",
    "        'left': 'center',\n",
    "        'bottom': '25%'\n",
    "    },\n",
    "    'series': [\n",
    "        {\n",
    "            'name': 'Punch Card',\n",
    "            'type': 'heatmap',\n",
    "            'data': [[0, i, d] for i, d in enumerate(data)],\n",
    "            'label': {\n",
    "                'show': True,\n",
    "                'position': 'inside',\n",
    "            },\n",
    "            'emphasis': {\n",
    "                'itemStyle': {\n",
    "                    'shadowBlur': 10,\n",
    "                    'shadowColor': 'rgba(0, 0, 0, 0.5)'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "options['series'][0]['data'] = [[0, i, d] for i, d in enumerate(data)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holley",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
